{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prompts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformrecognizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentAnalysisClient\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentintelligence\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyzeResult\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m df_transform\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_gpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_completion\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud_storage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CloudStorage\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prompts'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "\n",
    "#from prompts.prompts import df_transform\n",
    "from shared.transformer.chat_gpt import get_completion\n",
    "from shared.cloud_storage import CloudStorage\n",
    "from shared.loaders.bigquery_loader import Loader\n",
    "from shared.extractors.api.bigquery_extractor import Extractor\n",
    "\n",
    "\n",
    "endpoint = \"https://utadeo.cognitiveservices.azure.com/\"\n",
    "key = os.getenv(\"AZURE_KEY\")\n",
    "\n",
    "def corregir_comillas(diccionario):\n",
    "    for clave, valor in diccionario.items():\n",
    "        if isinstance(valor, str) and \"'\" in valor:\n",
    "            diccionario[clave] = valor.replace(\"'\", \"\")\n",
    "    return diccionario\n",
    "\n",
    "\n",
    "def analyze_layout(path_to_sample_documents):\n",
    "    document_intelligence_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    with open(path_to_sample_documents, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-invoice\", document=f, locale=\"en-US\"\n",
    "        )\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    if result.tables:\n",
    "        all_tables = pd.DataFrame()\n",
    "        for table_idx, table in enumerate(result.tables):\n",
    "            print(f\"Table # {table_idx} has {table.row_count} rows and {table.column_count} columns\")\n",
    "            if table.row_count > 3:\n",
    "                table_data = {}\n",
    "                for cell in table.cells:\n",
    "                    row_index = cell.row_index\n",
    "                    column_index = cell.column_index\n",
    "                    content = cell.content\n",
    "                    if row_index not in table_data:\n",
    "                        table_data[row_index] = {}\n",
    "                    table_data[row_index][column_index] = content\n",
    "\n",
    "                df = pd.DataFrame.from_dict(table_data, orient=\"index\").sort_index(axis=1)\n",
    "                df = df.apply(lambda x: x.str.replace('\\n', ''))\n",
    "                df = df.apply(lambda x: x.str.replace(\"'\", ''))\n",
    "                df = df.apply(lambda x: x.str.replace('\"', ''))\n",
    "                print(df)\n",
    "                results = get_completion(df_transform.format(dataframe=df))\n",
    "                print(results)\n",
    "                data_list = ast.literal_eval(results)\n",
    "                df_lista = pd.DataFrame(data_list)\n",
    "                all_tables = pd.concat([all_tables, df_lista], ignore_index=True)\n",
    "        all_tables = all_tables.applymap(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "        all_tables['source']=path_to_sample_documents\n",
    "        return all_tables\n",
    "\n",
    "def main():\n",
    "    cloud = CloudStorage()\n",
    "    lista = cloud.list_files_in_gcs(\"pdfs_utadeo\", \"pdfs/\")\n",
    "    print(lista)\n",
    "    lista.remove(\"pdfs/\")\n",
    "    df_storage = pd.DataFrame()\n",
    "    df_storage[\"file_path\"] = lista\n",
    "    df_storage[\"flag\"] = 0\n",
    "\n",
    "    Extract = Extractor()\n",
    "    df_bigquery = Extract.extract_bigquery(\"artful-sled-419501.secop.pdf_control\")\n",
    "    nuevos_pdfs = df_storage[~df_storage[\"file_path\"].isin(df_bigquery[\"file_path\"])]\n",
    "    \n",
    "    load = Loader()\n",
    "    if not nuevos_pdfs.empty:\n",
    "        load.load_bigquery_df(nuevos_pdfs, \"artful-sled-419501.secop.pdf_control\", \"WRITE_APPEND\")\n",
    "\n",
    "    df_bigquery_new = Extract.extract_bigquery_flag(\"artful-sled-419501.secop.pdf_control\")\n",
    "    if not df_bigquery_new.empty:\n",
    "        for row in df_bigquery_new.itertuples():\n",
    "            print('processing the file' + row.file_path)\n",
    "            destino = f\"/home/juanstn/{row.file_path.split('/')[-1]}\"\n",
    "            cloud.download_blob(\"pdfs_utadeo\", row.file_path, destino)\n",
    "            df_to_load = analyze_layout(destino)\n",
    "    return df_to_load\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_load = main()\n",
    "    print(df_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 126 rows and 7 columns to artful-sled-419501.secop.detalle_contratos\n"
     ]
    }
   ],
   "source": [
    "load = Loader()\n",
    "load.load_bigquery_df(df_load, \"artful-sled-419501.secop.detalle_contratos\", \"WRITE_APPEND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01T00:00:00.000 2021-01-07T00:00:00.000\n",
      "2021-01-08T00:00:00.000 2021-01-14T00:00:00.000\n",
      "2021-01-15T00:00:00.000 2021-01-21T00:00:00.000\n",
      "2021-01-22T00:00:00.000 2021-01-28T00:00:00.000\n",
      "2021-01-29T00:00:00.000 2021-02-04T00:00:00.000\n",
      "2021-02-05T00:00:00.000 2021-02-11T00:00:00.000\n",
      "2021-02-12T00:00:00.000 2021-02-18T00:00:00.000\n",
      "2021-02-19T00:00:00.000 2021-02-25T00:00:00.000\n",
      "2021-02-26T00:00:00.000 2021-03-04T00:00:00.000\n",
      "2021-03-05T00:00:00.000 2021-03-11T00:00:00.000\n",
      "2021-03-12T00:00:00.000 2021-03-18T00:00:00.000\n",
      "2021-03-19T00:00:00.000 2021-03-25T00:00:00.000\n",
      "2021-03-26T00:00:00.000 2021-04-01T00:00:00.000\n",
      "2021-04-02T00:00:00.000 2021-04-08T00:00:00.000\n",
      "2021-04-09T00:00:00.000 2021-04-15T00:00:00.000\n",
      "2021-04-16T00:00:00.000 2021-04-22T00:00:00.000\n",
      "2021-04-23T00:00:00.000 2021-04-29T00:00:00.000\n",
      "2021-04-30T00:00:00.000 2021-05-06T00:00:00.000\n",
      "2021-05-07T00:00:00.000 2021-05-13T00:00:00.000\n",
      "2021-05-14T00:00:00.000 2021-05-20T00:00:00.000\n",
      "2021-05-21T00:00:00.000 2021-05-27T00:00:00.000\n",
      "2021-05-28T00:00:00.000 2021-06-03T00:00:00.000\n",
      "2021-06-04T00:00:00.000 2021-06-10T00:00:00.000\n",
      "2021-06-11T00:00:00.000 2021-06-17T00:00:00.000\n",
      "2021-06-18T00:00:00.000 2021-06-24T00:00:00.000\n",
      "2021-06-25T00:00:00.000 2021-07-01T00:00:00.000\n",
      "2021-07-02T00:00:00.000 2021-07-08T00:00:00.000\n",
      "2021-07-09T00:00:00.000 2021-07-15T00:00:00.000\n",
      "2021-07-16T00:00:00.000 2021-07-22T00:00:00.000\n",
      "2021-07-23T00:00:00.000 2021-07-29T00:00:00.000\n",
      "2021-07-30T00:00:00.000 2021-08-05T00:00:00.000\n",
      "2021-08-06T00:00:00.000 2021-08-12T00:00:00.000\n",
      "2021-08-13T00:00:00.000 2021-08-19T00:00:00.000\n",
      "2021-08-20T00:00:00.000 2021-08-26T00:00:00.000\n",
      "2021-08-27T00:00:00.000 2021-09-02T00:00:00.000\n",
      "2021-09-03T00:00:00.000 2021-09-09T00:00:00.000\n",
      "2021-09-10T00:00:00.000 2021-09-16T00:00:00.000\n",
      "2021-09-17T00:00:00.000 2021-09-23T00:00:00.000\n",
      "2021-09-24T00:00:00.000 2021-09-30T00:00:00.000\n",
      "2021-10-01T00:00:00.000 2021-10-07T00:00:00.000\n",
      "2021-10-08T00:00:00.000 2021-10-14T00:00:00.000\n",
      "2021-10-15T00:00:00.000 2021-10-21T00:00:00.000\n",
      "2021-10-22T00:00:00.000 2021-10-28T00:00:00.000\n",
      "2021-10-29T00:00:00.000 2021-11-04T00:00:00.000\n",
      "2021-11-05T00:00:00.000 2021-11-11T00:00:00.000\n",
      "2021-11-12T00:00:00.000 2021-11-18T00:00:00.000\n",
      "2021-11-19T00:00:00.000 2021-11-25T00:00:00.000\n",
      "2021-11-26T00:00:00.000 2021-12-02T00:00:00.000\n",
      "2021-12-03T00:00:00.000 2021-12-09T00:00:00.000\n",
      "2021-12-10T00:00:00.000 2021-12-16T00:00:00.000\n",
      "2021-12-17T00:00:00.000 2021-12-23T00:00:00.000\n",
      "2021-12-24T00:00:00.000 2021-12-30T00:00:00.000\n",
      "2021-12-31T00:00:00.000 2022-01-06T00:00:00.000\n",
      "2022-01-07T00:00:00.000 2022-01-13T00:00:00.000\n",
      "2022-01-14T00:00:00.000 2022-01-20T00:00:00.000\n",
      "2022-01-21T00:00:00.000 2022-01-27T00:00:00.000\n",
      "2022-01-28T00:00:00.000 2022-02-03T00:00:00.000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def create_weeks(initial_date, last_date):\n",
    "    initial_date = datetime.datetime.strptime(initial_date, \"%Y-%m-%d\")\n",
    "    last_date = datetime.datetime.strptime(last_date, \"%Y-%m-%d\")\n",
    "    weeks = []\n",
    "    while initial_date < last_date:\n",
    "        week_start = initial_date.strftime(\"%Y-%m-%d\")\n",
    "        week_end = (initial_date + datetime.timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
    "        weeks.append({\n",
    "            \"initial_date\": week_start + \"T00:00:00.000\",\n",
    "            \"last_date\": week_end + \"T00:00:00.000\"\n",
    "        })\n",
    "        initial_date = initial_date + datetime.timedelta(days=7)\n",
    "    return weeks\n",
    "\n",
    "\n",
    "for week in create_weeks(\"2021-01-01\", \"2022-01-31\"):\n",
    "    initial_date = week[\"initial_date\"]\n",
    "    last_date = week[\"last_date\"]\n",
    "    print(initial_date, last_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
